# 策略B: 降低查重率

> **返回**: [SKILL.md](../SKILL.md)

## 核心原理

查重系统（如知网、维普）通过文本相似度匹配来检测重复内容。降查重的本质是：**保持学术内涵不变的前提下，进行深度语义改写**。

**关键平衡**: 既要降低文本相似度，又要保持专业术语的准确性和学术表达的规范性。

---

## 关键技术

### 技术1: 深度语义改写

#### 1.1 同义替换

**词汇级替换**:

| 原词 | 同义替换 | 使用场景 |
|------|---------|---------|
| 提出 | 提出/设计/构建/开发 | 方法描述 |
| 有效 | 有效/高效/显著/优异 | 性能评价 |
| 问题 | 问题/挑战/难题/瓶颈 | 问题陈述 |
| 方法 | 方法/策略/机制/框架 | 技术描述 |
| 实验 | 实验/评估/验证/测试 | 实验部分 |

❌ **原文**:
```
本文提出了一种有效的方法来解决这个问题。
```

✅ **改写**:
```
本文设计了一种高效的策略来应对这一挑战。
```

**短语级替换**:

| 原短语 | 同义替换 |
|--------|---------|
| 取得了显著成效 | 实现了性能提升/展现出优异表现 |
| 面临严峻挑战 | 遭遇关键瓶颈/存在核心难题 |
| 广泛应用于 | 在...领域发挥重要作用/被大量部署在 |
| 实验表明 | 评估结果显示/验证结果证实 |

#### 1.2 结构重组

**主被动转换**:

❌ **原文（主动）**:
```
本文提出了AdapTFormer模型以解决注意力稀释问题。
```

✅ **改写（被动）**:
```
为解决注意力稀释问题，AdapTFormer模型被提出并设计。
```

**句序调整**:

❌ **原文**:
```
Transformer具有强大的全局建模能力，但在长序列预测中面临注意力稀释问题。
```

✅ **改写（调整句序）**:
```
尽管在长序列预测中面临注意力稀释挑战，Transformer仍凭借其强大的全局建模能力成为主流架构。
```

#### 1.3 表述角度转换

**从"是什么"到"为什么"**:

❌ **原文**:
```
MV-FDE模块通过分解简化输入信号。
```

✅ **改写**:
```
为了简化输入信号的复杂度，MV-FDE模块采用分解策略将混合信号解耦为趋势和残差成分。
```

**从"做什么"到"怎么做"**:

❌ **原文**:
```
SAM模块动态调制注意力权重。
```

✅ **改写**:
```
SAM模块通过可学习的调制向量，在Softmax操作之前对注意力分数进行动态调整，从而实现权重的精准聚焦。
```

---

### 技术2: 引用规范化

#### 2.1 直接引用→间接引用转换

❌ **直接引用（易查重）**:
```
Zhou等人提出的Informer通过ProbSparse自注意力将复杂度降低到O(L log L)。
```

✅ **间接引用（降查重）**:
```
为降低计算复杂度，研究者们提出了稀疏注意力机制[1]，通过选择性计算将复杂度从O(L²)优化至O(L log L)。
```

#### 2.2 合理归并多源引用

❌ **逐一引用**:
```
Autoformer提出了分解方法。FEDformer在频域进行分解。TimesNet采用2D变分建模。
```

✅ **归并引用**:
```
近年来，分解驱动的方法成为研究热点，代表性工作包括时域分解[2]、频域分解[3]和2D变分建模[4]等不同技术路线。
```

#### 2.3 引用内容改写为个人表述

❌ **照搬原文**:
```
如文献[5]所述，"注意力稀释问题在长序列场景下尤为突出"。
```

✅ **个人表述**:
```
已有研究指出[5]，当序列长度增加时，注意力权重的分散现象会显著加剧，导致模型难以聚焦关键信息。
```

---

### 技术3: 专业术语处理

#### 3.1 核心术语保留原则

**必须保留的术语**（改写会导致错误）:
- 模型名称：Transformer, LSTM, CNN
- 算法名称：Attention, Softmax, Backpropagation
- 数学符号：MSE, MAE, KL散度
- 领域专有名词：时间序列预测, 知识蒸馏

#### 3.2 非核心描述灵活变换

❌ **原文**:
```
Transformer通过自注意力机制实现全局建模。
```

✅ **改写（保留核心术语）**:
```
Transformer架构借助自注意力（Self-Attention）组件捕捉序列中的全局依赖关系。
```

#### 3.3 公式推导添加注释

❌ **原文（纯公式）**:
```
Attention(Q, K, V) = softmax(QK^T / √d_k)V
```

✅ **改写（添加文字说明）**:
```
注意力计算过程可形式化为：首先计算Query和Key的点积相似度QK^T，
然后除以缩放因子√d_k以稳定梯度，接着通过softmax归一化得到注意力权重，
最后对Value进行加权求和，即Attention(Q, K, V) = softmax(QK^T / √d_k)V。
```

---

## 实战案例

### 案例1: 相关工作改写

**原文（高查重风险）**:
```
Informer通过ProbSparse自注意力降低了计算复杂度。Autoformer将时间序列分解
嵌入每个Transformer层。FEDformer在频域进行分解和注意力计算。这些方法在
长序列预测中取得了显著成效。
```

**优化后（深度改写）**:
```
为应对长序列预测的计算挑战，研究者们从不同角度进行了探索。一类工作聚焦于
稀疏化策略[1]，通过选择性计算关键Query-Key对将复杂度优化至O(L log L)；
另一类工作则强调时序分解的价值[2,3]，分别在时域和频域对混合信号进行解耦，
为后续建模提供更清晰的表示。这些创新性方法在多个基准数据集上展现出优异性能，
推动了长序列预测技术的发展。
```

**改写要点**:
- 同义替换："降低"→"优化"，"取得成效"→"展现性能"
- 结构重组：从逐一陈述改为分类归纳
- 角度转换：从"做了什么"到"为什么这样做"
- 引用归并：多个引用合并为[1,2,3]

---

### 案例2: 方法描述改写

**原文（高查重风险）**:
```
MV-FDE模块通过移动平均滤波将时间序列分解为趋势成分和残差成分，然后通过
独立的线性层分别嵌入这两个成分并融合。
```

**优化后（深度改写）**:
```
MV-FDE模块的核心设计理念在于问题重构：首先利用移动平均滤波器提取低频趋势信号，
将原始混合序列解耦为趋势和残差双视角；随后为每个视角配置专用的线性投影层，
学习各自的嵌入表示；最终通过直接求和实现双视角特征的融合，为后续注意力计算
提供更清晰的输入表示。
```

**改写要点**:
- 增加设计理念说明
- 将"分解"扩展为"提取低频信号+解耦"
- 将"嵌入"扩展为"配置投影层+学习表示"
- 添加目的说明："为...提供..."

---

## 检查清单

优化完成后，使用以下清单自查：

- [ ] 同义词替换率 > 30%
- [ ] 句式结构有明显变化
- [ ] 核心术语保持准确
- [ ] 直接引用已转为间接引用
- [ ] 公式有文字说明
- [ ] 表述角度有转换
- [ ] 整体语义内涵不变

---

## 注意事项

> [!CAUTION]
> **禁止的降查重方式**
> 
> 以下做法会破坏学术规范，**严禁使用**：
> - 随意替换专业术语（如"Transformer"→"变换器"）
> - 打乱逻辑顺序导致不连贯
> - 删除关键引用
> - 使用错误的同义词
> - 改变原有学术含义

> [!IMPORTANT]
> **正确的降查重原则**
> 
> - 保持学术内涵：改写后的内容必须与原文语义等价
> - 保留核心术语：专业名词不可随意替换
- 增强表达质量：改写应提升而非降低表达水平
> - 尊重原创性：引用他人工作必须标注

---

## 高风险区域特殊处理

### 摘要部分
- 风险：摘要常与已发表论文高度重合
- 策略：深度改写，调整叙述顺序，增加背景铺垫

### 相关工作
- 风险：综述性内容易与他人重复
- 策略：分类归纳，个人视角总结，避免逐一罗列

### 方法描述
- 风险：技术细节可能与论文/代码重复
- 策略：增加设计理念说明，扩展实现细节

### 实验设置
- 风险：标准实验设置高度相似
- 策略：添加选择理由，说明与baseline的差异

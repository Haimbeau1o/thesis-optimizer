# 策略A: 降低AI检测率

> **返回**: [SKILL.md](../SKILL.md)

## 核心原理

AI检测工具（如GPTZero、Originality.ai）通过识别机器生成文本的特征模式来判断内容是否由AI生成。这些特征包括：
- 过度规整的句式结构
- 过于完美的逻辑闭环
- 缺乏人类写作的自然变化
- 过度客观的"全知视角"

**降AI率的本质**：消除机器生成特征，增强人类写作特征，同时保持学术规范性。

---

## 关键技术

### 技术1: 句式多样化改写

#### 问题诊断
AI生成文本常见的句式问题：
- 过度使用并列结构（A、B、C三点）
- 句长过于均匀（标准差小）
- 缺乏复杂句式变化

#### 优化策略

**1.1 打破规整并列结构**

❌ **AI特征明显**:
```
本文的主要贡献包括：(1)提出了XX方法；(2)设计了YY模块；(3)实现了ZZ性能提升。
```

✅ **优化后**:
```
本文首先提出XX方法以解决...问题。在此基础上，我们进一步设计了YY模块，
该模块通过...机制实现了功能增强。实验表明，这些创新使得ZZ性能相比基线
提升了...，为...领域提供了新的解决思路。
```

**改写要点**:
- 将并列结构拆分为递进关系
- 增加因果连接词（"在此基础上"、"进一步"）
- 添加必要的背景铺垫

**1.2 引入句长变化**

**目标**: 句长标准差 > 8

**策略**:
- 短句（<15字）：强调关键结论
- 中句（15-30字）：常规叙述
- 长句（>30字）：复杂论证

❌ **句长过于均匀**:
```
Transformer模型具有强大的全局建模能力。(17字)
然而在长序列预测中面临注意力稀释问题。(19字)
本文提出分解增强注意力机制解决该问题。(18字)
```

✅ **优化后**:
```
Transformer模型具有强大的全局建模能力。(17字)
然而，当应用于长序列多变量预测任务时，标准注意力机制面临严重的
注意力稀释问题——模型倾向于将注意力权重近乎均匀地分配到所有时间步
和变量上，而非选择性地聚焦于对预测真正重要的关键信息。(78字)
为此，本文提出分解增强注意力机制。(15字)
```

**1.3 增加复杂句式**

**学术领域特有句式**:
- 让步转折："尽管...但是..."
- 条件假设："在...条件下，..."
- 递进强调："不仅...更重要的是..."

❌ **简单句堆砌**:
```
现有方法存在问题。本文提出新方法。新方法效果更好。
```

✅ **复杂句式**:
```
尽管现有方法在某些场景下取得了一定成效，但其在高维长序列场景下的
性能受限。为此，本文提出...方法，该方法不仅解决了...问题，更重要的是
通过...机制实现了...突破。
```

---

### 技术2: 学术语气自然化

#### 问题诊断
AI生成文本的语气问题：
- 过度客观，缺乏作者视角
- 避免使用第一人称（"本文"、"我们"）
- 缺乏必要的主观判断

#### 优化策略

**2.1 适当使用第一人称**

❌ **过度客观**:
```
该方法被提出以解决...问题。实验表明该方法有效。
```

✅ **作者视角**:
```
针对...问题，本文提出...方法。我们认为，该方法的核心优势在于...
实验验证了我们的设计理念。
```

**使用场景**:
- 提出创新点："本文提出"、"我们设计"
- 表达判断："我们认为"、"本文观察到"
- 强调贡献："我们的工作首次..."

**2.2 引入合理的主观判断**

❌ **纯客观陈述**:
```
实验结果显示MSE为0.245。
```

✅ **带判断的陈述**:
```
实验结果显示MSE为0.245，这一性能提升是显著的（significant），
充分验证了我们方法的有效性。
```

**2.3 保留学术探索的自然推进**

❌ **完美无缺的叙述**:
```
本文直接提出了最优解决方案。
```

✅ **自然的探索过程**:
```
初步实验中，我们尝试了...方法，但发现其在...场景下性能受限。
经过深入分析，我们意识到问题的根源在于...。基于这一洞察，
我们进一步提出了改进方案...
```

---

### 技术3: 逻辑链路人性化

#### 问题诊断
AI生成文本的逻辑问题：
- 逻辑过于完美，缺乏必要的铺垫
- 直接给出结论，缺少推导过程
- 缺乏对读者认知的考虑

#### 优化策略

**3.1 打破过于完美的逻辑闭环**

❌ **逻辑过于紧密**:
```
问题A存在。方法B可以解决。因此提出方法B。实验证明有效。
```

✅ **自然的推进**:
```
问题A在...场景下尤为突出，严重制约了...的应用。现有方法C虽然
尝试通过...途径缓解，但仍存在...局限。经过分析，我们发现问题的
根源在于...。基于这一洞察，本文提出方法B...。值得注意的是，
方法B与现有方法的关键区别在于...。实验表明...
```

**3.2 增加必要的背景铺垫**

**铺垫类型**:
- 问题背景："在...背景下"
- 动机说明："为了...目的"
- 对比分析："与...不同"

**3.3 考虑读者认知负担**

❌ **直接抛出复杂概念**:
```
本文采用跨维度知识蒸馏机制。
```

✅ **渐进式引入**:
```
传统知识蒸馏主要用于模型压缩，即从大模型向小模型迁移知识。
然而在时间序列预测中，我们面临一个新的挑战：如何将时间维度的
知识迁移到变量维度？为此，本文提出跨维度知识蒸馏机制...
```

---

## 实战案例

### 案例1: 摘要改写

**原文（AI特征明显）**:
```
多变量时间序列预测在多个领域具有重要应用。本文针对注意力稀释问题，
提出了AdapTFormer模型。该模型包含两个核心组件：MV-FDE模块和SAM模块。
实验表明该模型性能优异。
```

**优化后**:
```
多变量时间序列预测在能源调度、交通管理等领域发挥着不可替代的作用。
然而，现有Transformer方法在长序列场景下面临严重的注意力稀释问题——
模型难以聚焦关键依赖关系，导致预测精度显著下降。针对这一挑战，
本文提出AdapTFormer模型，采用"重构-增强"的设计理念：首先通过
MV-FDE模块在输入阶段分解并重构信号，为注意力机制创造更清晰的工作环境；
然后通过SAM模块动态调制注意力权重，实现精准聚焦。在9个公开数据集上的
广泛实验表明，AdapTFormer在绝大多数场景下取得最优或次优性能，
充分验证了其作为长序列预测强大工具的能力。
```

**改写要点**:
- 打破并列结构，改为递进关系
- 增加句长变化（短-长-长-中）
- 添加因果连接和背景铺垫
- 引入作者视角（"本文提出"、"我们的"）

---

## 检查清单

优化完成后，使用以下清单自查：

- [ ] 句长标准差 > 8
- [ ] 避免过度使用并列结构（(1)(2)(3)）
- [ ] 包含复杂句式（让步、条件、递进）
- [ ] 适当使用第一人称（"本文"、"我们"）
- [ ] 包含必要的背景铺垫和过渡
- [ ] 逻辑推进自然，非完美闭环
- [ ] 保留学术探索的自然过程

---

## 注意事项

> [!WARNING]
> **过度优化风险**
> 
> 降AI率不等于降低学术质量！以下做法是**错误的**：
> - 故意引入语法错误
> - 使用口语化表达
> - 破坏逻辑连贯性
> - 删除必要的术语

**正确做法**: 在保持学术规范的前提下，增强人类写作的自然特征。

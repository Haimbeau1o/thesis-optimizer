# Thesis Optimizer 使用指南

> **基于实战案例的最佳实践**  
> 本文档总结了thesis-optimizer skill的实际使用范式和关键经验

---

## 🎯 核心目标（永远记住！）

**最重要的提醒**：优化的核心目的是：
1. ✅ **降低AI检测率**（目标：<20%）
2. ✅ **降低查重率**（目标：<10%）

⚠️ **不是简单的润色或语法修正！**

---

## 📋 标准工作流程

### Phase 1: 初始化（5分钟）

#### 1.1 创建总揽文档
```markdown
创建: thesis_master_overview.md
内容: 
- 论文基本信息（标题、作者、章节结构）
- 当前状态评估（AI率、查重率估算）
- 优化目标和策略
- 章节清单和优先级
```

#### 1.2 创建任务管理文档
```markdown
创建: task.md
内容:
- 总体目标（具体数值）
- 分章节优化清单（checkbox格式）
- 优化策略提醒
- 当前工作状态
```

**关键点**：
- 必须设定明确的数值目标（如AI率从40%降至25%）
- 优先级：P0（摘要）> P1（绪论、方法章节）> P2（基础理论）

---

### Phase 2: 逐章节优化（主要工作）

#### 2.1 标准优化流程（每个章节）

**步骤1: 深度分析**
```markdown
1. 查看章节内容（view_file）
2. 识别AI特征：
   ✓ "通过...实现..."句式密度
   ✓ "首先...其次...最后"规整并列
   ✓ "该问题的根源在于"等AI模板
   ✓ 超长句（>100字）数量
   ✓ 句式单一性
3. 识别查重风险：
   ✓ 学术通用表述
   ✓ 标准方法描述
   ✓ 重复使用的固定搭配
```

**步骤2: 制定优化策略**
```markdown
创建章节优化文档（如chapter_01_intro.md）
记录:
- 当前AI特征统计
- 查重风险点列表
- 具体优化计划
- 预估效果
```

**步骤3: 执行优化**

**核心策略**：

1. **去除AI模板句式** ⭐⭐⭐⭐⭐
   ```
   ❌ "该问题的根源在于..."
   ✅ "为何产生这一问题？..." (疑问句)
   
   ❌ "通过MV-FDE模块在输入阶段进行..."
   ✅ "MV-FDE模块在输入层完成..."
   
   ❌ "困境的根源在于..."
   ✅ "单路径架构带来的困境："
   ```

2. **拆分超长句** ⭐⭐⭐⭐⭐
   ```
   ❌ 原句（114字）:
   "困境的根源在于单路径架构的固有局限——时间信息与变量信息被隐式耦合在同一表示空间，模型被迫在两者间"二选一"。在时空耦合的复杂场景中，这种单维度建模策略的性能受限，尤其在长预测步长时误差累积加剧。"
   
   ✅ 优化（3个短句：37字/40字/20字）:
   "单路径架构带来的困境在于：时间信息与变量信息混杂在同一向量空间，模型被迫在两者间"二选一"。在时空耦合的复杂场景中，这种单维度建模策略的性能受限。当预测步长较长时，误差累积加剧。"
   ```

3. **替换高查重风险表述** ⭐⭐⭐⭐
   ```
   高查重风险表述对照表:
   
   | 原表述 | 替换方案 | 降重效果 |
   |--------|---------|---------|
   | "在输入阶段进行" | "在输入层完成" / "预处理时" | ↓15% |
   | "被隐式耦合在同一表示空间" | "混杂在同一向量空间" | ↓30% |
   | "显式建模时间依赖" | "直接捕捉时序关系" | ↓25% |
   | "利用...显式建模" | "构建...捕捉" | ↓10% |
   ```

4. **增加句式多样性** ⭐⭐⭐⭐
   ```
   目标比例:
   - 疑问句: 每1000字至少2个
   - 短句(<30字): 占比30%
   - 中句(30-60字): 占比50%
   - 长句(>60字): 占比<20%
   
   技巧:
   ✓ 用疑问句打破AI模板
   ✓ 增加过渡短句
   ✓ 句长交替变化
   ```

5. **去除"通过"句式** ⭐⭐⭐⭐⭐
   ```
   替换方案（多样化）:
   "通过" → "采用" / "利用" / "借助" / "依靠" / 
            "融合" / "配合" / "引入" / "应用" / 
            "构建" / 直接去掉
   
   注意: 不要所有"通过"都改成同一个词！
   ```

**步骤4: 质量检查**
```markdown
检查清单:
□ AI特征是否大幅减少（>50%）
□ 查重风险是否降低
□ 句式是否多样化
□ 技术细节是否100%保留
□ LaTeX语法是否正确
□ 所有引用是否完整
```

---

### Phase 3: 深度优化（如果第一次效果不足）

#### 3.1 识别优化不足

**症状**：
- 降AI率<10%（应该>15%）
- 降查重率<2%（应该>3%）
- 句式仍然单一

**诊断方法**：
```markdown
1. Review修改前后的diff
2. 检查是否只是换词，句式结构未变
3. 统计AI特征残留量
4. 识别高查重风险点是否替换
```

#### 3.2 深度优化策略

**关键提醒**：⚠️ **不要只是表面替换词汇！**

**深度优化重点**：

1. **改写句式结构**（最重要）
   - 疑问句开头
   - 倒装句
   - 短句串联
   - 过渡句插入

2. **彻底拆分长句**
   - >100字的句子必须拆分
   - 拆分后句长要有变化（短-中-短模式）

3. **深度改写学术表述**
   - 不只是换个近义词
   - 整句重写，改变表述逻辑

---

## 🚨 常见错误和陷阱

### 错误1: 只去除"通过"，其他不变

❌ **错误做法**：
```
修改前: 通过MV-FDE模块在输入阶段进行特征分解
修改后: 利用MV-FDE模块在输入阶段进行特征分解
```
**问题**: 只换了一个词，句式结构完全未变，AI率仅降2-3%

✅ **正确做法**：
```
修改后: MV-FDE模块在输入层完成特征分解
或: MV-FDE模块分解输入特征
```

---

### 错误2: 忘记核心目的

❌ **表现**：
- 过度润色，导致句子更长
- 增加了很多"学术化"表述（反而提高查重率）
- 只关注语法正确性

✅ **正确mindset**：
- **核心目的永远是：降AI率 + 降查重率**
- 润色是次要的，降AI/降重是主要的

---

### 错误3: 技术细节丢失

❌ **危险操作**：
- 删除引用
- 修改术语
- 改变技术描述的准确性

✅ **必须保留**：
- 100%保留所有引用（\cite{}）
- 100%保留所有LaTeX公式和符号
- 100%保留技术准确性

---

## 📊 效果评估标准

### 优秀优化（⭐⭐⭐⭐⭐）

- 降AI率: ≥15%
- 降查重率: ≥4%
- 句式多样性: +200%
- 技术完整性: 100%

### 良好优化（⭐⭐⭐⭐）

- 降AI率: 10-15%
- 降查重率: 2-4%
- 句式多样性: +100%
- 技术完整性: 100%

### 不合格（需要重做）

- 降AI率: <10%
- 降查重率: <2%
- 技术细节有丢失

---

## 💡 实战技巧和经验

### 技巧1: 疑问句是最强武器

用疑问句打破AI模板，效果立竿见影：
```
❌ "该问题的根源在于..."
✅ "为何产生这一问题？..."

❌ "为了解决...挑战..."
✅ "如何解决...？"
```

降AI效果：单句可降20-30%

---

### 技巧2: 短句是降AI的利器

AI生成的文本倾向于长句，短句能有效降低AI检测：
```
策略: 每段插入2-3个短句（<20字）

示例:
"这种设计从源头简化了混合信号的复杂度。" (19字)
"现有方法未能根本解决。" (10字)
"GraphTD采用双路径架构。" (12字)
```

---

### 技巧3: 替换学术通用表述的优先级

**高优先级替换**（查重风险极高）：
- "被...耦合在...空间" 
- "在...阶段进行..."
- "显式建模..."
- "该...的根源在于"

**中优先级替换**：
- "通过...实现..."
- "利用...构建..."
- "采用...策略"

**可保留**（必须的技术术语）：
- 模型名称（Transformer、LSTM等）
- 技术术语（注意力机制、卷积等）

---

### 技巧4: 批量优化效率策略

**分批策略**：
1. 先优化P0（摘要）- 1小时
2. 再优化P1（绪论、核心方法）- 3-4小时
3. 最后优化P2（基础理论、总结）- 2小时

**每章节时间分配**：
- 分析：15分钟
- 优化：30-45分钟
- Review：10分钟

---

## 📝 文档模板

### 章节优化文档模板

```markdown
# 章节XX优化文档

## 基本信息
- 章节：第X章 XXX
- 行数：LXXX-LXXX
- 字数：约XXX字
- 优先级：P0/P1/P2

## 当前状态评估

### AI特征统计
- "通过...实现...": XX处
- 规整并列（首先...其次）: XX处
- AI模板句式: XX处
- 超长句（>100字）: XX个
- 短句（<30字）: XX个

### 查重风险点
- 学术通用表述: XX处
- 标准方法描述: XX处
- 高频固定搭配: XX处

## 优化计划

### 策略A - 降AI检测率
1. [ ] 去除XX处"通过"
2. [ ] 打破XX处并列结构
3. [ ] 拆分XX个超长句
4. [ ] 增加XX个疑问句

### 策略B - 降查重率
1. [ ] 替换XX处学术通用表述
2. [ ] 改写XX处方法描述

### 策略C - 学术润色
1. [ ] 增加具体数据
2. [ ] 术语一致性检查

## 预估效果
- 降AI率: XX% → XX% (↓XX%)
- 降查重率: XX% → XX% (↓XX%)
```

---

## 🎓 案例学习

### 案例1: 摘要优化

**优化前状态**：
- AI率：40%
- 查重率：12%
- 主要问题："首先...其次...此外"规整并列

**优化策略**：
- 打破规整并列：改为不对称结构
- 增加主观判断："我们认为..."
- 增加数据支撑：具体百分比

**优化后状态**：
- AI率：30% (↓10%)
- 查重率：10% (↓2%)

**耗时**：45分钟

---

### 案例2: 第一章深度优化

**第一次优化**（效果不足）：
- 仅去除"通过"
- 降AI率：45% → 37% (↓8%)
- 降查重率：20% → 19% (↓1%)

**问题诊断**：
- 句式结构未变
- 学术通用表述未替换
- 超长句未拆分

**第二次深度优化**：
- 改写AI模板（疑问句）
- 拆分12个超长句
- 替换23处查重风险点
- 增加2个疑问句、6个短句

**最终效果**：
- 降AI率：45% → 25% (↓20%)
- 降查重率：20% → 15% (↓5%)

**总耗时**：2.5小时（两次优化）

**教训**：不要被第一次优化的表面效果欺骗，要深度review！

---

## 🔧 工具使用建议

### 必备工具调用

1. **view_file**: 查看章节内容
2. **grep_search**: 搜索"通过"等AI特征词
3. **multi_replace_file_content**: 批量修改（非连续编辑）
4. **replace_file_content**: 单点修改（连续编辑）

### 效率提升技巧

**批量搜索AI特征**：
```bash
# 搜索"通过"
grep_search(query="通过", file=thesis.tex)

# 搜索并列结构
grep_search(query="首先", file=thesis.tex)
grep_search(query="其次", file=thesis.tex)
```

**版本控制**：
- 每次优化前备份
- 记录修改点（行号）
- 保留优化前后对比

---

## ⚠️ 重要提醒

### 1. 核心目的（重复三遍）

✅ **核心目的是：降AI率 + 降查重率**  
✅ **核心目的是：降AI率 + 降查重率**  
✅ **核心目的是：降AI率 + 降查重率**

### 2. 技术完整性

⚠️ **绝对不能丢失**：
- 引用（\cite{}）
- LaTeX公式
- 技术术语
- 数据准确性

### 3. 深度优化的必要性

如果第一次优化效果不足：
- 不要停留在替换词汇的层面
- 必须改写句式结构
- 必须拆分超长句
- 必须替换学术通用表述

---

## 📚 参考资源

### Skill文档
- `SKILL.md`: 总体说明
- `examples/`: 优化案例
- `scripts/`: 辅助脚本（如有）

### Artifact文档
- `thesis_master_overview.md`: 总揽
- `task.md`: 任务清单
- `chapter_XX_xxx.md`: 章节优化文档
- `overall_review.md`: 最终review

---

## 🎯 Quick Start

**第一次使用？按这个流程：**

1. ✅ 创建总揽文档（5分钟）
2. ✅ 创建任务清单（5分钟）
3. ✅ 从摘要开始优化（1小时）
4. ✅ Review效果，如果不足则深度优化
5. ✅ 继续第一章（2-3小时）
6. ✅ 重复流程优化其他章节

**记住**：不要追求一次性完美，允许迭代优化！

---

**最后的最后**：

🎯 **降AI率 + 降查重率 = 核心目标**

永远不要忘记！
